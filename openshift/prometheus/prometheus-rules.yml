# these rules synced manually from https://github.com/etcd-io/etcd/blob/master/Documentation/etcd-mixin/mixin.libsonnet
groups:
- name: etcd
  rules:
  - alert: etcdInsufficientMembers
    annotations:
      message: 'etcd cluster "{{ $labels.job }}": insufficient members ({{ $value
        }}).'
    expr: |
      sum(up{job=~".*etcd.*"} == bool 1) by (job) < ((count(up{job=~".*etcd.*"}) by (job) + 1) / 2)
    for: 3m
    labels:
      severity: critical
  - alert: etcdNoLeader
    annotations:
      message: 'etcd cluster "{{ $labels.job }}": member {{ $labels.instance }} has
        no leader.'
    expr: |
      etcd_server_has_leader{job=~".*etcd.*"} == 0
    for: 1m
    labels:
      severity: critical
  - alert: etcdHighNumberOfLeaderChanges
    annotations:
      message: 'etcd cluster "{{ $labels.job }}": instance {{ $labels.instance }}
        has seen {{ $value }} leader changes within the last hour.'
    expr: |
      rate(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}[15m]) > 3
    for: 15m
    labels:
      severity: warning
#  - alert: etcdHighNumberOfFailedGRPCRequests
#    annotations:
#      message: 'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for {{
#        $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
#    expr: |
#      100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code!="OK"}[5m])) BY (job, instance, grpc_service, grpc_method)
#        /
#      sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) BY (job, instance, grpc_service, grpc_method)
#        > 1
#    for: 10m
#    labels:
#      severity: warning
#  - alert: etcdHighNumberOfFailedGRPCRequests
#    annotations:
#      message: 'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for {{
#        $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
#    expr: |
#      100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code!="OK"}[5m])) BY (job, instance, grpc_service, grpc_method)
#        /
#      sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) BY (job, instance, grpc_service, grpc_method)
#        > 5
#    for: 5m
#    labels:
#      severity: critical
  - alert: etcdGRPCRequestsSlow
    annotations:
      message: 'etcd cluster "{{ $labels.job }}": gRPC requests to {{ $labels.grpc_method
        }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
    expr: |
      histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~".*etcd.*", grpc_type="unary"}[5m])) by (job, instance, grpc_service, grpc_method, le))
      > 0.15
    for: 10m
    labels:
      severity: critical
  - alert: etcdMemberCommunicationSlow
    annotations:
      message: 'etcd cluster "{{ $labels.job }}": member communication with {{ $labels.To
        }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
    expr: |
      histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m]))
      > 0.15
    for: 10m
    labels:
      severity: warning
  - alert: etcdHighNumberOfFailedProposals
    annotations:
      message: 'etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures within
        the last hour on etcd instance {{ $labels.instance }}.'
    expr: |
      rate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[15m]) > 5
    for: 15m
    labels:
      severity: warning
  - alert: etcdHighFsyncDurations
    annotations:
      message: 'etcd cluster "{{ $labels.job }}": 99th percentile fync durations are
        {{ $value }}s on etcd instance {{ $labels.instance }}.'
    expr: |
      histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
      > 0.5
    for: 10m
    labels:
      severity: warning
  - alert: etcdHighCommitDurations
    annotations:
      message: 'etcd cluster "{{ $labels.job }}": 99th percentile commit durations
        {{ $value }}s on etcd instance {{ $labels.instance }}.'
    expr: |
      histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
      > 0.25
    for: 10m
    labels:
      severity: warning
  - alert: etcdHighNumberOfFailedHTTPRequests
    annotations:
      message: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd
        instance {{ $labels.instance }}'
    expr: |
      sum(rate(etcd_http_failed_total{job=~".*etcd.*", code!="404"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job=~".*etcd.*"}[5m]))
      BY (method) > 0.01
    for: 10m
    labels:
      severity: warning
  - alert: etcdHighNumberOfFailedHTTPRequests
    annotations:
      message: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd
        instance {{ $labels.instance }}.'
    expr: |
      sum(rate(etcd_http_failed_total{job=~".*etcd.*", code!="404"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job=~".*etcd.*"}[5m]))
      BY (method) > 0.05
    for: 10m
    labels:
      severity: critical
  - alert: etcdHTTPRequestsSlow
    annotations:
      message: etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method
        }} are slow.
    expr: |
      histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m]))
      > 0.15
    for: 10m
    labels:
      severity: warning
#ceph rules 
- name: ceph.rules
  rules:
  - alert: CephTargetDown
    expr: up{job="ceph"} == 0
    for: 5s 
    labels:
      severity: critical
    annotations:
      description: CEPH target down for more than 2m, please check - it could be a either exporter crash or a whole cluster crash
      summary: CEPH exporter down
  - alert: CephErrorState
    expr: ceph_health_status > 1
    for: 5m
    labels:
      severity: critical
    annotations:
      description: Ceph is in Error state longer than 5m, please check status of pools and OSDs
      summary: CEPH in ERROR
  - alert: CephWarnState
    expr: ceph_health_status == 1
    for: 30m
    labels:
      severity: warning
    annotations:
      description: Ceph is in Warn state longer than 30m, please check status of pools and OSDs
      summary: CEPH in WARN
  - alert: OsdDown
    expr: ceph_osd_up == 0
    for: 30m
    labels:
      severity: warning
    annotations:
      description: OSD is down longer than 30 min, please check whats the status
      summary: OSD down
  - alert: OsdApplyLatencyTooHigh
    expr: ceph_osd_perf_apply_latency_seconds > 10
    for: 90s
    labels:
      severity: warning
    annotations:
      description: OSD latency for {{ $labels.osd }} is too high. Please check if it doesn't stuck in weird state
      summary: OSD latency too high {{ $labels.osd }}
  - alert: MonitorClockSkewTooHigh
    expr: abs(ceph_monitor_clock_skew_seconds) > 0.1
    for: 60s
    labels:
      severity: warning
    annotations:
      description: Monitor clock skew detected on  {{ $labels.monitor }} - please check ntp and harware clock settins
      summary: Clock skew detected on {{ $labels.monitor }}
  - alert: MonitorAvailableStorage
    expr: ceph_monitor_avail_percent < 30
    for: 60s
    labels:
      severity: warning
    annotations:
      description: Monitor storage for {{ $labels.monitor }} less than 30% - please check why its too high
      summary: Nonitor storage for  {{ $labels.monitor }} less than 30%
  - alert: MonitorAvailableStorage
    expr: ceph_monitor_avail_percent < 15
    for: 60s
    labels:
      severity: critical
    annotations:
      description: Monitor storage for {{ $labels.monitor }} less than 15% - please check why its too high
      summary: Nonitor storage for  {{ $labels.monitor }} less than 15%
  - alert: CephOSDUtilizatoin
    expr: ceph_osd_utilization > 90
    for: 60s
    labels:
      severity: critical
    annotations:
      description: Osd free space for  {{ $labels.osd }} is higher tan 90%. Please validate why its so big, reweight or add storage
      summary: OSD {{ $labels.osd }} is going out of space
  - alert: CephPgDown
    expr: ceph_pg_down > 0
    for: 3m
    labels:
      severity: critical
    annotations:
      description: Some groups are down (unavailable) for too long on {{ $labels.cluster }}. Please ensure that all the data are available
      summary: PG DOWN [{{ $value }}] on {{ $labels.cluster }}
  - alert: CephPgIncomplete
    expr: ceph_pg_incomplete > 0
    for: 2m
    labels:
      severity: critical
    annotations:
      description: Some groups are incomplete (unavailable) for too long on {{ $labels.cluster }}. Please ensure that all the data are available
      summary: PG INCOMPLETE [{{ $value }}] on {{ $labels.cluster }}
  - alert: CephPgInconsistent
    expr: ceph_pg_inconsistent > 0
    for: 1m
    labels:
      severity: warning
    annotations:
      description: Some groups are inconsistent for too long on {{ $labels.cluster }}. Data is available but inconsistent across nodes
      summary: PG INCONSISTENT [{{ $value }}] on {{ $labels.cluster }}
  - alert: CephPgActivating
    expr: ceph_pg_activating > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      description: Some groups are activating for too long on {{ $labels.cluster }}. Those PGs are unavailable for too long!
      summary: PG ACTIVATING [{{ $value }}] on {{ $labels.cluster }}
  - alert: CephPgBackfillTooFull
    expr: ceph_pg_backfill_toofull > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      description: Some groups are located on full OSD on cluster {{ $labels.cluster }}. Those PGs can be unavailable shortly. Please check OSDs, change weight or reconfigure CRUSH rules.
      summary: PG TOO FULL [{{ $value }}] on {{ $labels.cluster }}
  - alert: CephPgUnavailable
    expr: ceph_pg_total - ceph_pg_active > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      description: Some groups are unavailable on {{ $labels.cluster }}. Please check their detailed status and current configuration.
      summary: PG UNAVAILABLE [{{ $value }}] on {{ $labels.cluster }}
  - alert: CephOsdReweighted
    expr: ceph_osd_weight < 1
    for: 1h
    labels:
      severity: warning
    annotations:
      description: OSD {{ $labels.ceph_daemon}} on cluster {{ $labels.cluster}} was reweighted for too long. Please either create silent or fix that issue
      summary: OSD {{ $labels.ceph_daemon }} on {{ $labels.cluster }} reweighted - {{ $value }}
